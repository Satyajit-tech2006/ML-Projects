ğŸ“˜ Simple Linear Regression â€“ Practical Recall Notes
1ï¸âƒ£ Why X must be 2D and y can be 1D

In scikit-learn:

X = features â†’ shape (n_samples, n_features) â†’ 2D

y = target â†’ shape (n_samples, ) â†’ 1D

Why?

ML models are designed for multiple features

Even if there is one feature, sklearn expects the same interface

X = df[['Weight']]   # âœ… 2D DataFrame
y = df['Height']     # âœ… 1D Series


âŒ Using df['Weight'] as X breaks consistency.

Mental rule:

One sample = one row, one feature = one column â†’ always 2D.

2ï¸âƒ£ Why random_state = 42
train_test_split(X, y, test_size=0.25, random_state=42)

Why?

Train-test split is random

Without random_state, every run gives different split

That causes:

Different model

Different metrics

Confusing results

random_state = fixed seed â†’ reproducibility

ğŸ“Œ 42 is just a convention. Any number works.

3ï¸âƒ£ Why Trainâ€“Test Split is mandatory

Goal: Test model on unseen data

If you train and test on same data:

Model looks perfect

Actually useless in real world

This is called overfitting illusion

Rule:

Train â†’ Learn patterns
Test â†’ Check generalization

4ï¸âƒ£ Why Standardization is used
Formula (z-score)
ğ‘§
=
ğ‘¥
âˆ’
ğœ‡
ğœ
z=
Ïƒ
xâˆ’Î¼
	â€‹

Why standardize?

Gradient descent converges faster

Prevents features with large scale from dominating

Improves numerical stability

âš ï¸ For OLS (closed-form):

Standardization is not mandatory

But still a good habit

âš ï¸ For Gradient Descent:

Standardization is very important

5ï¸âƒ£ Why fit_transform on train but only transform on test
scaler.fit_transform(X_train)  # âœ…
scaler.transform(X_test)       # âœ…

Why?

fit â†’ learns mean & std

transform â†’ applies them

âŒ If you fit on test data:

Test data leaks into training

Results become invalid

This is called data leakage

Golden rule:

Never learn anything from test data.

6ï¸âƒ£ Why sklearn LinearRegression works without gradient descent

LinearRegression() uses OLS (Normal Equation) internally.

That means:

No iterations

No learning rate

Direct mathematical solution

Equation:

ğœƒ
=
(
ğ‘‹
ğ‘‡
ğ‘‹
)
âˆ’
1
ğ‘‹
ğ‘‡
ğ‘¦
Î¸=(X
T
X)
âˆ’1
X
T
y

Thatâ€™s why:

Scaling not strictly required

Very fast for small datasets

7ï¸âƒ£ Meaning of Coefficient and Intercept
regressor.coef_       # slope (Î²â‚)
regressor.intercept_  # intercept (Î²â‚€)


Equation:

ğ‘¦
^
=
ğ›½
0
+
ğ›½
1
ğ‘¥
y
^
	â€‹

=Î²
0
	â€‹

+Î²
1
	â€‹

x

Coefficient â†’ change in y for 1 unit change in x

Intercept â†’ predicted y when x = 0
(may not be meaningful physically, but mathematically needed)

8ï¸âƒ£ Why Visualization is important

Scatter plot â†’ checks if linear regression is valid

Line plot â†’ confirms model fit visually

Helps catch:

Non-linearity

Outliers

Bad assumptions

ğŸ“Œ ML is visual + mathematical

9ï¸âƒ£ Error Metrics â€“ When to use what

MAE â†’ average absolute mistake (easy to interpret)

MSE â†’ penalizes large errors more

RMSE â†’ same unit as target (preferred)

Lower = better.

ğŸ”Ÿ RÂ² vs Adjusted RÂ²
RÂ²

% of variance explained

Always increases with more features

Adjusted RÂ²

Penalizes unnecessary features

Better for model comparison

Use:

RÂ² â†’ basic understanding

Adjusted RÂ² â†’ model selection

1ï¸âƒ£1ï¸âƒ£ Why predict new data must be standardized

If model trained on standardized data:

new_x = scaler.transform([[72]])
regressor.predict(new_x)


âŒ Using raw input gives wrong output

Rule:

Same preprocessing for train, test, and future data.

1ï¸âƒ£2ï¸âƒ£ sklearn vs OLS (statsmodels)
sklearn	statsmodels
Fast	Detailed statistics
Production-ready	Research/analysis
Less stats	Full statistical summary

Both give same coefficients â†’ confirms correctness.