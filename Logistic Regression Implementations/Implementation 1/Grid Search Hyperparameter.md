14ï¸âƒ£ Grid Search Hyperparameter Tuning â€“ Reminder Notes
=======================================================

1ï¸âƒ£ Why Hyperparameter Tuning?
------------------------------

*   Default parameters â‰  best parameters
    
*   Different **random\_state â†’ different split â†’ different accuracy**
    
*   Goal: **find the most stable + best-performing model**
    

2ï¸âƒ£ What Are Hyperparameters?
-----------------------------

*   Parameters **not learned** by model
    
*   Set **before training**
    
*   Control **biasâ€“variance tradeoff**
    

Examples in Logistic Regression:

*   penalty
    
*   C
    
*   solver
    
*   class\_weight
    

3ï¸âƒ£ Meaning of Important Logistic Regression Params
---------------------------------------------------

ParameterReminderpenaltyType of regularization (l1, l2, elasticnet)CInverse of regularization strength (â†“C = â†‘regularization)solverOptimization algorithmclass\_weightHandle imbalance

4ï¸âƒ£ Why GridSearchCV?
---------------------

> Tries **ALL possible combinations** of parametersSelects the **best one using cross-validation**

âœ” ExhaustiveâŒ Slower for large grids

5ï¸âƒ£ Parameter Grid (Most Important Step)
----------------------------------------

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   params = {      'penalty': ['l1', 'l2'],      'C': [100, 10, 1, 0.1, 0.01],      'solver': ['liblinear', 'saga']  }   `

ðŸ“Œ **Keys must EXACTLY match model parameter names**

6ï¸âƒ£ Why Not Any Solver With Any Penalty?
----------------------------------------

Some combinations are **invalid**

*   liblinear â†’ supports l1, l2
    
*   saga â†’ supports l1, l2, elasticnet
    

ðŸ“Œ Always check sklearn docs

7ï¸âƒ£ Cross-Validation (Why Needed?)
----------------------------------

*   Single train-test split is unreliable
    
*   CV gives **robust performance estimate**
    

8ï¸âƒ£ Stratified K-Fold (Very Important)
--------------------------------------

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   from sklearn.model_selection import StratifiedKFold  cv = StratifiedKFold(n_splits=5)   `

Why stratified?

*   Keeps **class ratio same** in every fold
    
*   Prevents biased evaluation
    

9ï¸âƒ£ Creating GridSearchCV Object
--------------------------------

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   from sklearn.model_selection import GridSearchCV  grid = GridSearchCV(      estimator=model,      param_grid=params,      scoring='accuracy',      cv=cv,      n_jobs=-1  )   `

### What Each Argument Means

ArgumentMeaningestimatorModel to tuneparam\_gridParameters to tryscoringMetric to optimizecvCross-validation strategyn\_jobs=-1Use all CPU cores

ðŸ”Ÿ Training Grid Search
-----------------------

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   grid.fit(X_train, y_train)   `

What happens internally:

*   Try every param combo
    
*   Apply CV for each
    
*   Store scores
    
*   Pick best combo
    

1ï¸âƒ£1ï¸âƒ£ Getting Best Parameters & Score
--------------------------------------

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   grid.best_params_  grid.best_score_   `

ðŸ“Œ best\_score\_ â†’ CV score (not test score)

1ï¸âƒ£2ï¸âƒ£ Making Predictions with Best Model
-----------------------------------------

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   y_pred = grid.predict(X_test)   `

ðŸ“Œ Uses **best\_estimator\_ internally**

1ï¸âƒ£3ï¸âƒ£ Why GridSearchCV Improves Performance?
---------------------------------------------

*   Reduces overfitting
    
*   Finds optimal regularization
    
*   Balances bias & variance
    

1ï¸âƒ£4ï¸âƒ£ When NOT to Use GridSearchCV?
------------------------------------

*   Huge datasets
    
*   Very large parameter space
    

ðŸ‘‰ Use **RandomizedSearchCV** instead (next topic)

ðŸ§  One-Line Mental Model
------------------------

> **GridSearchCV = brute-force + cross-validated parameter optimization**